{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "496d8de7",
   "metadata": {},
   "source": [
    "# 文本表示\n",
    "\n",
    "文本表示：将文本数据表示成计算机能够运算的数字或向量。\n",
    "\n",
    "在自然语言处理（Natural Language Processing，NLP）领域，文本表示是处理流程的第一步，主要是将文本转换为计算机可以运算的数字。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7c0f6f",
   "metadata": {},
   "source": [
    "## 01 词袋模型(Bag of Words)\n",
    "\n",
    "- 思想：\n",
    "\n",
    "  把每篇文章看成一袋子词，并忽略每个词出现的顺序。具体来看：将整段文本表示成一个长向量，每一维代表一个单词。该维对应的权重代表这个词在原文章中的重要程度。\n",
    "\n",
    "- 例子1：\n",
    "\n",
    "  句1：Jane wants to go to Shenzhen 句2：Bob wants to go to Shanghai\n",
    "\n",
    "  使用两个例句来构造词袋： [Jane, wants, to, go, Shenzhen, Bob, Shanghai]\n",
    "\n",
    "  两个例句就可以用以下两个向量表示，对应的下标与映射数组的下标相匹配，其值为该词语出现的次数\n",
    "\n",
    "  句1：[1,1,2,1,1,0,0]  句2：[0,1,2,1,0,1,1]\n",
    "\n",
    "- 例子2：\n",
    "\n",
    "  这次我们加上停用词和标点符号的处理，\n",
    "\n",
    "  句1：Jane wants to go to Shenzhen . 句2：Bob wants to go to Shanghai , me too ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa1f094d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jane', 'wants', 'to', 'go', 'to', 'Shenzhen', '.']\n",
      "['Bob', 'wants', 'to', 'go', 'to', 'Shanghai', ',', 'me', 'too', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence1 = 'Jane wants to go to Shenzhen .'\n",
    "sentence2 = 'Bob wants to go to Shanghai , me too .'\n",
    "\n",
    "tokens1 = sentence1.split(\" \")\n",
    "tokens2 = sentence2.split(\" \")\n",
    "print(tokens1)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c5a3a6",
   "metadata": {},
   "source": [
    "### 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27df55a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tokens, filtered_vocab):\n",
    "    \"\"\"\n",
    "    向量化\n",
    "    \"\"\"\n",
    "    vector = []\n",
    "    for w in filtered_vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e8d609",
   "metadata": {},
   "source": [
    "### 去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c767f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jane',\n",
       " 'wants',\n",
       " 'to',\n",
       " 'go',\n",
       " 'Shenzhen',\n",
       " '.',\n",
       " 'Bob',\n",
       " 'Shanghai',\n",
       " ',',\n",
       " 'me',\n",
       " 'too']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unique(sequence):\n",
    "    \"\"\"\n",
    "    去重\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "# create a vocabulary list\n",
    "vocab = unique(tokens1+tokens2)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e879b7",
   "metadata": {},
   "source": [
    "使用两个例句的tokens，过滤停用词和标点符号后来构造有效词袋："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78225ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jane', 'wants', 'go', 'Shenzhen', 'Bob', 'Shanghai', 'me', 'too']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 停用词\n",
    "stopwords = [\"to\", \"is\", \"a\"]\n",
    "# 标点符号\n",
    "special_chars = [\",\", \":\", \";\", \".\", \"?\"]\n",
    "\n",
    "# 过滤停用词和标点符号\n",
    "filtered_vocab = []\n",
    "for w in vocab: \n",
    "    if w not in stopwords and w not in special_chars: \n",
    "        filtered_vocab.append(w)\n",
    "filtered_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "520b8c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 0, 0, 0, 0]\n",
      "[0, 1, 1, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# convert sentences into vectords\n",
    "vector1 = vectorize(tokens1, filtered_vocab)\n",
    "print(vector1)\n",
    "vector2 = vectorize(tokens2, filtered_vocab)\n",
    "print(vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1b767a",
   "metadata": {},
   "source": [
    "Bag of Words模型向量的size就是vocabulary的size大小，所以该向量表示非常稀疏。\n",
    "\n",
    "\n",
    "下面演示使用sklearn库做Bag of Words模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bda0c71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bob</th>\n",
       "      <th>go</th>\n",
       "      <th>jane</th>\n",
       "      <th>me</th>\n",
       "      <th>shanghai</th>\n",
       "      <th>shenzhen</th>\n",
       "      <th>to</th>\n",
       "      <th>too</th>\n",
       "      <th>wants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bob  go  jane  me  shanghai  shenzhen  to  too  wants\n",
       "0    0   1     1   0         0         1   2    0      1\n",
       "1    1   1     0   1         1         0   2    1      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    " \n",
    "sentence1 = 'Jane wants to go to Shenzhen .'\n",
    "sentence2 = 'Bob wants to go to Shanghai , me too .'\n",
    "  \n",
    "count_vec = CountVectorizer(ngram_range=(1, 1), # to use bigrams ngram_range=(2,2)\n",
    "                           #stop_words='english'\n",
    "                           )\n",
    "#transform\n",
    "feature = count_vec.fit_transform([sentence1, sentence2])\n",
    " \n",
    "#create dataframe\n",
    "df = pd.DataFrame(feature.toarray(), columns=count_vec.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d12f25d",
   "metadata": {},
   "source": [
    "## 02 词频-逆向文件频率（TF-IDF）\n",
    "\n",
    "- 思想：\n",
    "\n",
    "  字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。\n",
    "\n",
    "- 公式：\n",
    "\n",
    "  - $TF-IDF(t,d)=TF(t,d) × IDF(t)$\n",
    "  - $IDF(t)=log\\frac {文章总数} {包含单词t的文章总数+1}$\n",
    "  - $TF=\\frac{单词t在文档中出现的次数}{该文档的总词量}$\n",
    "\n",
    "- 缺点：\n",
    "\n",
    "  （1）没有考虑特征词的位置因素对文本的区分度，词条出现在文档的不同位置时，对区分度的贡献大小是不一样的。\n",
    "\n",
    "  （2）按照传统TF-IDF，往往一些生僻词的IDF(反文档频率)会比较高、因此这些生僻词常会被误认为是文档关键词。\n",
    "\n",
    "  （3）IDF部分只考虑了特征词与它出现的文本数之间的关系，而忽略了特征项在一个类别中不同的类别间的分布情况。\n",
    "\n",
    "  （4）对于文档中出现次数较少的重要人名、地名信息提取效果不佳。\n",
    "\n",
    "使用示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fdd384c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.44493104 0.\n",
      "  0.54957835 0.         0.         0.         0.         0.\n",
      "  0.         0.54957835 0.         0.         0.         0.\n",
      "  0.         0.         0.44493104 0.        ]\n",
      " [0.39137817 0.         0.         0.         0.31685436 0.\n",
      "  0.         0.49641358 0.         0.         0.         0.\n",
      "  0.39137817 0.         0.         0.         0.         0.49641358\n",
      "  0.         0.         0.31685436 0.        ]\n",
      " [0.26805872 0.         0.33999849 0.33999849 0.         0.33999849\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.26805872 0.         0.33999849 0.33999849 0.         0.\n",
      "  0.33999849 0.33999849 0.21701663 0.        ]\n",
      " [0.         0.33999849 0.         0.         0.21701663 0.\n",
      "  0.26805872 0.         0.33999849 0.33999849 0.33999849 0.33999849\n",
      "  0.         0.26805872 0.         0.         0.33999849 0.\n",
      "  0.         0.         0.         0.33999849]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bob</th>\n",
       "      <th>discuss</th>\n",
       "      <th>disneyland</th>\n",
       "      <th>during</th>\n",
       "      <th>go</th>\n",
       "      <th>in</th>\n",
       "      <th>jane</th>\n",
       "      <th>me</th>\n",
       "      <th>month</th>\n",
       "      <th>next</th>\n",
       "      <th>...</th>\n",
       "      <th>shanghai</th>\n",
       "      <th>shenzhen</th>\n",
       "      <th>summer</th>\n",
       "      <th>the</th>\n",
       "      <th>tim</th>\n",
       "      <th>too</th>\n",
       "      <th>vacation</th>\n",
       "      <th>visit</th>\n",
       "      <th>wants</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.549578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.549578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444931</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.391378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316854</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.496414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.496414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316854</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.268059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339998</td>\n",
       "      <td>0.339998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339998</td>\n",
       "      <td>0.339998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339998</td>\n",
       "      <td>0.339998</td>\n",
       "      <td>0.217017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339998</td>\n",
       "      <td>0.339998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        bob   discuss  disneyland    during        go        in      jane  \\\n",
       "0  0.000000  0.000000    0.000000  0.000000  0.444931  0.000000  0.549578   \n",
       "1  0.391378  0.000000    0.000000  0.000000  0.316854  0.000000  0.000000   \n",
       "2  0.268059  0.000000    0.339998  0.339998  0.000000  0.339998  0.000000   \n",
       "3  0.000000  0.339998    0.000000  0.000000  0.217017  0.000000  0.268059   \n",
       "\n",
       "         me     month      next  ...  shanghai  shenzhen    summer       the  \\\n",
       "0  0.000000  0.000000  0.000000  ...  0.000000  0.549578  0.000000  0.000000   \n",
       "1  0.496414  0.000000  0.000000  ...  0.391378  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  ...  0.268059  0.000000  0.339998  0.339998   \n",
       "3  0.000000  0.339998  0.339998  ...  0.000000  0.268059  0.000000  0.000000   \n",
       "\n",
       "        tim       too  vacation     visit     wants      with  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.444931  0.000000  \n",
       "1  0.000000  0.496414  0.000000  0.000000  0.316854  0.000000  \n",
       "2  0.000000  0.000000  0.339998  0.339998  0.217017  0.000000  \n",
       "3  0.339998  0.000000  0.000000  0.000000  0.000000  0.339998  \n",
       "\n",
       "[4 rows x 22 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence3 = 'Bob wants to visit Disneyland in Shanghai during the summer vacation  .'\n",
    "sentence4 = 'Tim is planning to go to Shenzhen next month to discuss project with Jane .'\n",
    "contents = [sentence1, sentence2, sentence3, sentence4]\n",
    "# 参数为 CounterVectorizer 和 TfidfTransformer 的所有参数\n",
    "vec = TfidfVectorizer(stop_words=stopwords,\n",
    "                      norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "feature = vec.fit_transform(contents) #直接对文档进行转换提取tfidf特征\n",
    "#一步就得到了tfidf向量\n",
    "print(feature.toarray())\n",
    "#create dataframe\n",
    "df = pd.DataFrame(feature.toarray(), columns=vec.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4604fe30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
